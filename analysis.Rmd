---
title: "Twitter Immigrant Geographic Anger Analysis"
author: "James Bain"
date: "3/14/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**IMPORTANT NOTE**: Be sure to set the working directory to the `tweetgeoang/` directory before reading in any of the data.

#### a bit of housekeeping...
This analysis relies on several spatial packages and (`brms`)[https://cran.r-project.org/web/packages/brms/index.html], which is a handy package to run some Bayesian models using `R`'s formula syntax. It also relies on some data called from the (`haterzmapper`)[https://github.com/jcbain/haterzmapper] package. The following block of code will install `haterzmapper` if it isn't already installed. 

```{r}
## install haterzmapper package if it's not already installed
if (!requireNamespace("haterzmapper", quietly = TRUE)) {
  devtools::install_github("jcbain/haterzmapper")
}
```

...then load these packages along with reading in some helper fuctions specified in the `R/` subdirectory of this repo. 
```{r}
library(ggplot2)
library(readr)
library(dplyr)
library(sf)
library(rnaturalearth)
library(rnaturalearthdata)
library(haterzmapper)
library(geosphere)
library(brms)
library(spdep)
library(purrr)

# read in helper functions
source("R/cleaning_functions.R")
source("R/spatial_functions.R")
```

## The Data
The data here are anger classified tweets predicted by the gru tweet anger model. Each file corresponds to a specific keyword regarding immigration. These keywords are "refugee", "illegals" and "immigrant". Any tweet from our database containing these keywords will be in these files. A baseline subset was also randomly sampled to gather a baseline anger index per city. 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
ref_ang <- read_csv("data/refugee_anger.csv")
ill_ang <- read_csv("data/illegals_anger.csv")
immi_ang <- read_csv("data/immigrant_anger.csv")

gen_ang <- read_csv("data/baseline_anger.csv")
```


### Other Data
There is also some other data that is pertinent to this analysis. Things like Metropolitan Statistical Area ids, a mapping between msas and jobs and a simple features data frame of city centers.
```{r}
# get count data from msas from 
# ~/Documents/research/TweetingLocale/papers/city_scaling/analyses/diversity_scaling.Rmd
msa_counts <- read_csv("data/acs_msa_counts.csv") %>% mutate(msa_geoid = as.character(msa_geoid))
research_cities <- read_csv("data/research_cities.csv") 

# get msa id for each job
msa_jobs <- research_cities %>%
  left_join(haterzmapper::topcities) %>%
  arrange(msa_geoid)

# find msa_geoids and map to job_ids
msa_jobs %>% left_join(msa_counts) %>%
  arrange(rank) %>%
  distinct(msa_geoid, .keep_all = T)

# create an sf frame of city points
cities_sf <- topcities %>% 
  dplyr::select(rank, full_name, lon, lat, msa_geoid) %>% 
  rename(city_name = full_name) %>% 
  right_join(msa_jobs %>% distinct(msa_geoid)) %>% 
  arrange(rank) %>% 
  distinct(msa_geoid, .keep_all = T) %>%
  st_as_sf(coords = c("lon", "lat")) %>%
  st_set_crs(4326)
```

## Geographic Data
One idea is that the distance to the border could play a role in the amount of animosity we see in some of these cities with regard to immigration. There is at least a basis for a hypothesis given that there is a spatial demention to the anger index. 

We need to first find the border between Mexico and the U.S. Fortunately, this task is pretty simple with the `sf` package and teh `st_intersection` function.
```{r}
# get world data
world <- ne_countries(scale = "medium", returnclass = "sf") %>% st_set_crs(4326)

# filter out us and mexico
us <- world %>% filter(geounit == "United States of America") 
mexico <- world %>% filter(geounit == "Mexico") 

# find the border
border <- st_intersection(us, mexico)

# find the distance to the border
cities_sf$distance_border <- st_distance(cities_sf, border)[,1]/1000
cities_sf$distance_border <- as.numeric(cities_sf$distance_border)

# quick little plot to visualize the border
ggplot() + 
  geom_sf(data = us, color = '#3e3e3f', fill = '#e8e8ea') +
  geom_sf(data = mexico, color = '#3e3e3f', fill = '#e8e8ea', size = 0.25) + 
  geom_sf(data = border, color = '#dd13a4', size = 2) + 
  geom_sf(data = border, color = '#e3f939') + 
  annotate("text", x = -95, y = 32, label = "UNITED STATES", color = '#3e3e3f') + 
  annotate("text", x = -105, y = 26, label = "MEXICO", color = '#3e3e3f') + 
  annotate("text", x = -94, y = 27, label = "GULF OF MEXICO", color = '#d3d3d3', size = 2) +
  annotate("text", x = -117, y = 28, label = "PACIFIC", color = '#d3d3d3', size = 2) +
  theme_bw() + 
  coord_sf(xlim = c(-90, -120), ylim = c(24.5, 34), expand = FALSE)
```

To find the overall anger index for each these cities for each of these data sets involves summing up the anger class for each city divided by the number of tweets for that city. This value is then divided by 2 in order to create an index between 0 and 1 where 0 is no anger at all and 1 is complete anger in all tweets for that city. 
```{r}
ref_ang_summary <- create_summary_data(ref_ang)
immi_ang_summary <- create_summary_data(immi_ang)
ill_ang_summary <- create_summary_data(ill_ang)
gen_ang_summary <- create_summary_data(gen_ang)

# find the difference between baseline and index_anger
ref_ang_comp <- create_comparison(ref_ang_summary)
ill_ang_comp <- create_comparison(ill_ang_summary)
immi_ang_comp <- create_comparison(immi_ang_summary)

# oh and a full frame of all immigration related
ang <- bind_rows(ref_ang, immi_ang, ill_ang) %>% 
  distinct(tweet_id_str, .keep_all = T) %>%
  create_summary_data() %>% 
  create_comparison() %>% 
  mutate(msa_geoid = as.character(msa_geoid))

ang_sf <- ang %>% left_join(cities_sf)

# adjust the scalings of these to make the models a bit more easier to interpret 
# hun_kilo = each unit is 100 km
# ind_perc = percentage for of index_diff
ang_sf$hun_kilo <- ang_sf$distance_border/100
ang_sf$ind_perc <- ang_sf$index_diff * 100
```

# ANALYSIS

### Find the Scale Adjusted Metropolitan Indicators
These values are performance indicators of ubran indicators. In this case, the number of naturalized citizens, number of non-citizens and the number of Spanish speakers are considered. The relationship between these indicators and city population size are transformed on a log scale and a bayesian linear model is run to find the relationship between these variables. The residuals for these models are the scale adjusted metropolitan indicators and allow for inter-city comparisons independent of population size. Positive residuals mean that a city is above what is expected and negative means they are below. These scale adjusted indicators are used as predictors in the final models predicting the relative anger index.

```{r}
# establish data sets
msa_nat_citizen <- msa_counts %>% pick_msavars(B05001_005)
msa_spanish <- msa_counts %>% pick_msavars(B16001_003)
msa_non <- msa_counts %>% pick_msavars(B05001_006)
# set priors, here I'm thinking about diffuse priors that might work for you
priors <- c( set_prior("normal(1,10)", class = "b", coef = "logx"),
             set_prior("normal(0, 10)", class = "Intercept")
             # can also put a different prior on sigma here if you want
)
#create models
nat_mod <- brm(logy ~ logx, prior = priors, data = msa_nat_citizen, cores = 4)
span_mod <- brm(logy ~ logx, prior = priors, data = msa_spanish, cores = 4)
non_mod <- brm(logy ~ logx, prior = priors, data = msa_non, cores = 4)
# save residuals
msa_nat_citizen$resids <- residuals(nat_mod)[,1]
msa_spanish$resids <- residuals(span_mod)[,1]
msa_non$resids <- residuals(non_mod)[,1]
```

### Anger Index predicted by distance to the border and scale adjusted indicators


```{r}

# create a spatial weight matrix
coords <- create_coords(ang_sf %>% st_as_sf())
wnb <- find_spatial_neighbors(coords = coords)  %>% nb2listw()

# find moran i
moran.mc(ang_sf$ind_perc, wnb, nsim = 999)
# create a spatially lagged anger index from the weighted distance matrix
ang.lag <- lag.listw(wnb, ang_sf$ind_perc)
ang_sf$lag <- ang.lag

## Bayesian Linear Models 
# priors for the scale adjusted metropolitan indicator models
modresid_priors <- c(set_prior("normal(0, 1)", class = "b", coef = "hun_kilo"),
                     set_prior("normal(0, 10)", class = "b", coef = "resids"))

# check moran i with bayesian linear model
fit1 <- brm(lag ~ ind_perc, data = ang_sf,
            chains = 4, cores = 4)
hypothesis(fit1, "ind_perc > 0.1")


## NATURALIZED CITIZEN MODELS
nat_ang_sf <- combine_resids(msa_nat_citizen, ang_sf)
# percentage ajusted anger ~ hundred kilometers from border + naturalized 
# citizens residuals + spatial lag
nat_fit <- brm(ind_perc ~ hun_kilo + resids, data = nat_ang_sf,
            autocor = cor_lagsar(wnb),
            prior = modresid_priors,
            chains = 2, cores = 4)
# check for the spatial auto-correlation of the residuals
moran.mc(residuals(nat_fit)[,1], wnb, nsim = 999)

# interaction
nat_fit_inter <- brm(ind_perc ~ hun_kilo + resids + (hun_kilo * resids), data = nat_ang_sf,
            autocor = cor_lagsar(wnb),
            prior = modresid_priors,
            chains = 2, cores = 4)
moran.mc(residuals(nat_fit_inter)[,1], wnb, nsim = 999)

## NON-CITIZEN
non_ang_sf <- combine_resids(msa_non, ang_sf)
# percentage ajusted anger ~ hundred kilometers from border + non-citizens 
# residuals + spatial lag
not_fit <- brm(ind_perc ~ hun_kilo + resids, data = non_ang_sf,
            autocor = cor_lagsar(wnb),
            prior = modresid_priors,
            chains = 2, cores = 4)
moran.mc(residuals(non_fit)[,1], wnb, nsim = 999)

non_fit_inter <- brm(ind_perc ~ hun_kilo + resids + (hun_kilo * resids), data = non_ang_sf,
            autocor = cor_lagsar(wnb),
            prior = modresid_priors,
            chains = 2, cores = 4)
moran.mc(residuals(non_fit_inter)[,1], wnb, nsim = 999)

## SPANISH SPEAKERS
span_ang_sf <- combine_resids(msa_spanish, ang_sf)

# percentage ajusted anger ~ hundred kilometers from border + spanish speaking 
# residuals + spatial lag
span_fit <- brm(ind_perc ~ hun_kilo + resids, data = span_ang_sf,
            autocor = cor_lagsar(wnb),
            prior = modresid_priors,
            chains = 2, cores = 4)
moran.mc(residuals(span_fit)[,1], wnb, nsim = 999)

span_fit_inter <- brm(ind_perc ~ hun_kilo + resids + (hun_kilo * resids), data = span_ang_sf,
            autocor = cor_lagsar(wnb),
            prior = modresid_priors,
            chains = 2, cores = 4)
moran.mc(residuals(non_fit_inter)[,1], wnb, nsim = 999)

# see if interactions are above 0
hypothesis(non_fit_inter, "hun_kilo:resids > 0")
hypothesis(nat_fit_inter, "hun_kilo:resids < 0")
hypothesis(span_fit_inter, "hun_kilo:resids < 0")

# check the best models with leave one out 
loo(nat_fit, nat_fit_inter)
loo(non_fit, non_fit_inter)
loo(span_fit, span_fit_inter)
```
