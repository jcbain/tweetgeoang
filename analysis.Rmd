---
title: "Twitter Immigrant Geographic Anger Analysis"
author: "James Bain"
date: "3/14/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### a bit of housekeeping...
This analysis relies on several spatial packages and (`brms`)[https://cran.r-project.org/web/packages/brms/index.html], which is a handy package to run some Bayesian models using `R`'s formula syntax. It also relies on some data called from the (`haterzmapper`)[https://github.com/jcbain/haterzmapper] package. The following block of code will install `haterzmapper` if it isn't already installed. 

```{r}
## install haterzmapper package if it's not already installed
if (!requireNamespace("haterzmapper", quietly = TRUE)) {
  devtools::install_github("jcbain/haterzmapper")
}
```

...then load these packages along with reading in some helper fuctions specified in the `R/` subdirectory of this repo. 
```{r}
library(ggplot2)
library(readr)
library(dplyr)
library(sf)
library(rnaturalearth)
library(rnaturalearthdata)
library(haterzmapper)
library(geosphere)
library(brms)
library(spdep)
library(purrr)

# read in helper functions
source("R/cleaning_functions.R")
source("R/spatial_functions.R")
```

## The Data
The data here are anger classified tweets predicted by the gru tweet anger model. Each file corresponds to 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
ref_ang <- read_csv("~/Dropbox/tweets/tweetgeoang/refugee_anger.csv")
ill_ang <- read_csv("~/Dropbox/tweets/tweetgeoang/illegals_anger.csv")
immi_ang <- read_csv("~/Dropbox/tweets/tweetgeoang/immigrant_anger.csv")

gen_ang <- read_csv("~/Dropbox/tweets/tweetgeoang/baseline_anger.csv")
```


### Other Data
There is also some other data that is pertinent to this analysis. Things like Metropolitan Statistical Area ids, a mapping between msas and jobs and a simple features data frame of city centers.
```{r}
# get count data from msas from 
# ~/Documents/research/TweetingLocale/papers/city_scaling/analyses/diversity_scaling.Rmd
msa_counts <- read_csv("~/Dropbox/tweets/tweetgeoang/acs_msa_counts.csv") %>% mutate(msa_geoid = as.character(msa_geoid))
research_cities <- read_csv("~/Dropbox/tweets/tweetgeoang/research_cities.csv") 
# get msa id for each job
msa_jobs <- research_cities %>%
  left_join(haterzmapper::topcities) %>%
  arrange(msa_geoid)
# find msa_geoids and map to job_ids
msa_jobs %>% left_join(msa_counts) %>%
  arrange(rank) %>%
  distinct(msa_geoid, .keep_all = T)
# create an sf frame of city points
cities_sf <- topcities %>% 
  dplyr::select(rank, full_name, lon, lat, msa_geoid) %>% 
  rename(city_name = full_name) %>% 
  right_join(msa_jobs %>% distinct(msa_geoid)) %>% 
  arrange(rank) %>% 
  distinct(msa_geoid, .keep_all = T) %>%
  st_as_sf(coords = c("lon", "lat")) %>%
  st_set_crs(4326)
```

## Geographic Data
One idea is that the distance to the border could play a role in the amount of animosity we see in some of these cities with regard to immigration. There is at least a basis for a hypothesis given that there is a spatial demention to the anger index. 

We need to first find the border between Mexico and the U.S. Fortunately, this task is pretty simple with the `sf` package and teh `st_intersection` function.
```{r}
# get world data
world <- ne_countries(scale = "medium", returnclass = "sf") %>% st_set_crs(4326)
# filter out us and mexico
us <- world %>% filter(geounit == "United States of America") 
mexico <- world %>% filter(geounit == "Mexico") 
# find the border
border <- st_intersection(us, mexico)
# quick little plot
ggplot() + 
  geom_sf(data = us, color = '#3e3e3f', fill = '#e8e8ea') +
  geom_sf(data = mexico, color = '#3e3e3f', fill = '#e8e8ea', size = 0.25) + 
  geom_sf(data = border, color = '#dd13a4', size = 2) + 
  geom_sf(data = border, color = '#e3f939') + 
  annotate("text", x = -95, y = 32, label = "UNITED STATES", color = '#3e3e3f') + 
  annotate("text", x = -105, y = 26, label = "MEXICO", color = '#3e3e3f') + 
  annotate("text", x = -94, y = 27, label = "GULF OF MEXICO", color = '#d3d3d3', size = 2) +
  annotate("text", x = -117, y = 28, label = "PACIFIC", color = '#d3d3d3', size = 2) +
  theme_bw() + 
  coord_sf(xlim = c(-90, -120), ylim = c(24.5, 34), expand = FALSE)
```
It is now possible to find the distance from each city to the shared border between Mexico and the U.S. The distances will be in kilometer units.
```{r}
cities_sf$distance_border <- st_distance(cities_sf, border)[,1]/1000
cities_sf$distance_border <- as.numeric(cities_sf$distance_border)
```

```{r}
ref_ang_summary <- create_summary_data(ref_ang)
immi_ang_summary <- create_summary_data(immi_ang)
ill_ang_summary <- create_summary_data(ill_ang)
gen_ang_summary <- create_summary_data(gen_ang)
# find the difference between baseline and index_anger
ref_ang_comp <- create_comparison(ref_ang_summary)
ill_ang_comp <- create_comparison(ill_ang_summary)
immi_ang_comp <- create_comparison(immi_ang_summary)
# oh and a full frame of all immigration related
ang <- bind_rows(ref_ang, immi_ang, ill_ang) %>% 
  distinct(tweet_id_str, .keep_all = T) %>%
  create_summary_data() %>% 
  create_comparison() %>% 
  mutate(msa_geoid = as.character(msa_geoid))

ang_sf <- ang %>% left_join(cities_sf)
# adjust the scalings of these to make the models a bit more easier to interpret 
# hun_kilo = each unit is 100 km
# ind_perc = percentage for of index_diff
ang_sf$hun_kilo <- ang_sf$distance_border/100
ang_sf$ind_perc <- ang_sf$index_diff * 100
```

# ANALYSIS

## Regression of Urban Indicator Residuals against Anger Indices

The common hypothesis is social psychology is that the greater exposure one has to the "other" the more amicable the feelings become toward the "other." Well, one way to do this is to compare it with the data that we established in the paper 1, the scaling of diversity one.


### The Previous Models
We have to collect the residuals somehow, so we are going to select a few of the models that we ran in paper 1 that are more directly related to the issue of immigration nowadays. Those variables are teh counts of naturalized citizers, the number of spanish speakers and the number of non-citizens. 

```{r}
# establish data sets
msa_nat_citizen <- msa_counts %>% pick_msavars(B05001_005)
msa_spanish <- msa_counts %>% pick_msavars(B16001_003)
msa_non <- msa_counts %>% pick_msavars(B05001_006)
# set priors, here I'm thinking about diffuse priors that might work for you
priors <- c( set_prior("normal(1,10)", class = "b", coef = "logx"),
             set_prior("normal(0, 10)", class = "Intercept")
             # can also put a different prior on sigma here if you want
)
#create models
nat_mod <- brm(logy ~ logx, prior = priors, data = msa_nat_citizen, cores = 4)
span_mod <- brm(logy ~ logx, prior = priors, data = msa_spanish, cores = 4)
non_mod <- brm(logy ~ logx, prior = priors, data = msa_non, cores = 4)
# save residuals
msa_nat_citizen$resids <- residuals(nat_mod)[,1]
msa_spanish$resids <- residuals(span_mod)[,1]
msa_non$resids <- residuals(non_mod)[,1]
```


```{r}

# create a spatial weight matrix
coords <- create_coords(ang_sf %>% st_as_sf())
wnb <- find_spatial_neighbors(coords = coords)  %>% nb2listw()

# find moran i
moran.mc(ang_sf$ind_perc, wnb, nsim = 999)
# create a spatially lagged anger index from the weighted distance matrix
ang.lag <- lag.listw(wnb, ang_sf$ind_perc)
ang_sf$lag <- ang.lag

## Bayesian Linear Models 
# priors for the scale adjusted metropolitan indicator models
modresid_priors <- c(set_prior("normal(0, 1)", class = "b", coef = "hun_kilo"),
                     set_prior("normal(0, 10)", class = "b", coef = "resids"))

fit1 <- brm(lag ~ ind_perc, data = ang_sf,
            chains = 4, cores = 4)
hypothesis(fit1, "ind_perc > 0.1")
# percentage ajusted anger ~ spatial lag
# fit1 <- brm(ind_perc ~ 1, data = ang_sf,
#             autocor = cor_lagsar(wnb), 
#             chains = 2, cores = 4)

## NATURALIZED CITIZEN MODELS
nat_ang_sf <- combine_resids(msa_nat_citizen, ang_sf)
# percentage ajusted anger ~ hundred kilometers from border + naturalized 
# citizens residuals + spatial lag
nat_fit <- brm(ind_perc ~ hun_kilo + resids, data = nat_ang_sf,
            autocor = cor_lagsar(wnb),
            prior = modresid_priors,
            chains = 2, cores = 4)
# check for the spatial auto-correlation of the residuals
moran.mc(residuals(nat_fit)[,1], wnb, nsim = 999)

# interaction
nat_fit_inter <- brm(ind_perc ~ hun_kilo + resids + (hun_kilo * resids), data = nat_ang_sf,
            autocor = cor_lagsar(wnb),
            prior = modresid_priors,
            chains = 2, cores = 4)
moran.mc(residuals(nat_fit_inter)[,1], wnb, nsim = 999)

## NON-CITIZEN
non_ang_sf <- combine_resids(msa_non, ang_sf)
# percentage ajusted anger ~ hundred kilometers from border + non-citizens 
# residuals + spatial lag
not_fit <- brm(ind_perc ~ hun_kilo + resids, data = non_ang_sf,
            autocor = cor_lagsar(wnb),
            prior = modresid_priors,
            chains = 2, cores = 4)
moran.mc(residuals(non_fit)[,1], wnb, nsim = 999)

non_fit_inter <- brm(ind_perc ~ hun_kilo + resids + (hun_kilo * resids), data = non_ang_sf,
            autocor = cor_lagsar(wnb),
            prior = modresid_priors,
            chains = 2, cores = 4)
moran.mc(residuals(non_fit_inter)[,1], wnb, nsim = 999)

## SPANISH SPEAKERS
span_ang_sf <- combine_resids(msa_spanish, ang_sf)

# percentage ajusted anger ~ hundred kilometers from border + spanish speaking 
# residuals + spatial lag
span_fit <- brm(ind_perc ~ hun_kilo + resids, data = span_ang_sf,
            autocor = cor_lagsar(wnb),
            prior = modresid_priors,
            chains = 2, cores = 4)
moran.mc(residuals(span_fit)[,1], wnb, nsim = 999)

span_fit_inter <- brm(ind_perc ~ hun_kilo + resids + (hun_kilo * resids), data = span_ang_sf,
            autocor = cor_lagsar(wnb),
            prior = modresid_priors,
            chains = 2, cores = 4)
moran.mc(residuals(non_fit_inter)[,1], wnb, nsim = 999)

hypothesis(non_fit_inter, "hun_kilo:resids > 0")
hypothesis(nat_fit_inter, "hun_kilo:resids < 0")
hypothesis(span_fit_inter, "hun_kilo:resids < 0")

loo(nat_fit, nat_fit_inter)
loo(non_fit, non_fit_inter)
loo(span_fit, span_fit_inter)
```
